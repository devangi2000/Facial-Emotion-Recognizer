{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial Emotion Recognition","metadata":{}},{"cell_type":"markdown","source":"## importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.layers import Conv2D, Dense, BatchNormalization, Activation, Dropout, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\nimport datetime\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.image import load_img,img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESNET\nfrom keras.applications.resnet import ResNet101, ResNet152, ResNet50\n\n# VGG\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\n#from tensorflow.keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.applications.vgg19 import VGG19\n\n# INCEPTION\nfrom keras.applications.inception_v3 import InceptionV3\n# DENSENET\n#from tensorflow.keras.applications.denset import InceptionV3\n#from tensorflow.keras.applications.inception_v3 import InceptionV3\n# NASNET\n# XCEPTION\n# EFFICIENTNET\n# MOBILENET\n\nfrom keras.layers import Input, Dense, GlobalAveragePooling2D","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Dataset","metadata":{}},{"cell_type":"code","source":"train_dir = '../input/fer2013/train/'\ntest_dir = '../input/fer2013/test/'\n\nrow, col = 48, 48\nclasses = 7\n\ndef count_exp(path, set_):\n    dict_ = {}\n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n    df = pd.DataFrame(dict_, index=[set_])\n    return df\ntrain_count = count_exp(train_dir, 'train')\ntest_count = count_exp(test_dir, 'test')\nprint(train_count)\nprint(test_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PLot of number of images in training set","metadata":{}},{"cell_type":"code","source":"train_count.transpose().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PLot of number of images in test set","metadata":{}},{"cell_type":"code","source":"test_count.transpose().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,22))\ni = 1\nfor expression in os.listdir(train_dir):\n    img = load_img((train_dir + expression +'/'+ os.listdir(train_dir + expression)[1]))\n    plt.subplot(1,7,i)\n    plt.imshow(img)\n    plt.title(expression)\n    plt.axis('off')\n    i += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\ncpt=0\nfor expression in os.listdir(train_dir):\n    for i in range(1,8):\n        cpt += 1\n        plt.subplot(7,8,cpt)\n        img=load_img(train_dir+expression+\"/\"+os.listdir(train_dir+expression)[i],target_size=(48,48))\n        plt.imshow(img,cmap='gray')\n        #plt.xlabel(os.listdir(train_dir+expression)[i])\n        plt.title(expression)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP_EMOTIONS = [\"surprise\", \"fear\", \"angry\", \"neutral\", \"sad\", \"disgust\",\"happy\"]\ntotal_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vision Transformer","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = \"../input/fer2013/train/\"\nTEST_PATH = \"../input/fer2013/test/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_images = 0\nfor dir_ in os.listdir(TRAIN_PATH):\n    count = 0\n    for f in os.listdir(TRAIN_PATH + dir_ + \"/\"):\n        count += 1\n        total_images += 1\n    print(f\"{dir_} has {count} number of images\")\n    \nprint(f\"\\ntotal images are {total_images}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_arr = np.empty(shape=(total_images,48,48,3))\nimg_label = np.empty(shape=(total_images))\nlabel_to_text = {}\n\ni = 0\ne = 0\nfor dir_ in os.listdir(TRAIN_PATH):\n    if dir_ in TOP_EMOTIONS:\n        label_to_text[e] = dir_\n        for f in os.listdir(TRAIN_PATH + dir_ + \"/\"):\n            img_arr[i] = cv2.imread(TRAIN_PATH + dir_ + \"/\" + f)\n            img_label[i] = e\n            i += 1\n        print(f\"loaded all {dir_} images to numpy arrays\")\n        e += 1\n\nimg_arr.shape, img_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_label = np_utils.to_categorical(img_label)\nimg_label.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_arr = img_arr / 255.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=0.9, random_state=42)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U tensorflow-addons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 7\ninput_shape = (48,48, 3)\n\n#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n\n#print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n#print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 30\nimage_size = 72  # We'll resize input images to this size\npatch_size = 6  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.experimental.preprocessing.Normalization(),\n        layers.experimental.preprocessing.Resizing(image_size, image_size),\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n        layers.experimental.preprocessing.RandomZoom(\n            height_factor=0.2, width_factor=0.2\n        ),\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation.layers[0].adapt(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 4))\nimage = x_train[np.random.choice(range(x_train.shape[0]))]\nplt.imshow(image.astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(image_size, image_size)\n)\npatches = Patches(patch_size)(resized_image)\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    patches = Patches(patch_size)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_experiment(model):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    checkpoint_filepath = \"/tmp/checkpoint\"\n    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_split=0.1,\n        callbacks=[checkpoint_callback],\n    )\n\n    model.load_weights(checkpoint_filepath)\n    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U tensorflow==2.4.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit_classifier = create_vit_classifier()\nhistory = run_experiment(vit_classifier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Training and test sets","metadata":{}},{"cell_type":"markdown","source":"### Pre-trained Model","metadata":{}},{"cell_type":"code","source":"\nIMAGE_SIZE=[224,224]\ntrain_path='../input/fer2013/train'\n\nvalid_path='../input/fer2013/test'\n\nfolders= glob.glob('../input/fer2013/train/*')\nfolders","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res50 = ResNet50(weights='imagenet',input_shape=IMAGE_SIZE+[3],include_top=False)\n\nfor layer in res50.layers:\n    layer.trainable=False\nx= Flatten()(res50.output)\n\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel_res50 = Model(inputs=res50.input,outputs=prediction)\nmodel_res50.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks","metadata":{}},{"cell_type":"code","source":"chk_path = 'resnet50.h5'\nlog_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             mode='min',\n                             moniter='val_loss')\n\nearlystop = EarlyStopping(monitor='val_loss', \n                          min_delta=0, \n                          patience=3, \n                          restore_best_weights=True)\n                        \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=6, \n                              min_delta=0.0001)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"\n#model_res50.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size=(224,224),\n                                                 batch_size=128,\n                                                 #shuffle=True,\n                                                 #color_mode=\"grayscale\",\n                                                 class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size=(224,224),\n                                            batch_size=128,\n                                            #shuffle=False,\n                                            #color_mode=\"grayscale\",\n                                            class_mode='categorical')\ntraining_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsteps_per_epoch = len(training_set) // 128\nvalidation_steps = len(test_set) // 128\n\nhist_r50 = model_res50.fit_generator(training_set,\n                 validation_data=test_set\n                 epochs=60,\n                 callbacks=callbacks,\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history_r50 = model_res50.fit_generator(training_set,\n #                                       validation_data=test_set,\n  #                                      epochs=30,\n   #                                     steps_per_epoch=len(training_set),\n    #                                    validation_steps=len(test_set))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Model","metadata":{}},{"cell_type":"code","source":"def get_model(input_size, classes=7):\n     #Initialising the CNN\n    model = tf.keras.models.Sequential()   \n\n    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape =input_size))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(2, 2))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(classes, activation='softmax'))\n\n    #Compliling the model\n    model.compile(optimizer=Adam(lr=0.0001, decay=1e-6), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fernet = get_model((row,col,1), classes)\nfernet.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(fernet, to_file='fernet.png', show_shapes=True, show_layer_names=True)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nhistory1 = fernet.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = 50,\n                    callbacks = [callbacks])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NASNet","metadata":{}},{"cell_type":"code","source":"import keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nasnet = keras.applications.NASNetMobile(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in nasnet.layers:\n    layer.trainable=False\nx= Flatten()(nasnet.output)\n\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel_nasnet = Model(inputs=nasnet.input,outputs=prediction)\nmodel_nasnet.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfernet.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size=(224,224),\n                                                 batch_size=128,\n                                                 #shuffle=True,\n                                                 #color_mode=\"grayscale\",\n                                                 class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size=(224,224),\n                                            batch_size=128,\n                                            #shuffle=False,\n                                            #color_mode=\"grayscale\",\n                                            class_mode='categorical')\ntraining_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#steps_per_epoch = len(training_set) // 128\n#validation_steps = len(test_set) // 128\n\nhist_fernet = fernet.fit_generator(training_set,\n                 validation_data=test_set,\n                 #verbose=1,\n                 epochs=60,\n                 callbacks=callbacks,\n                 #steps_per_epoch=steps_per_epoch,\n                 validation_steps=test_set/256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EfficientNet B2","metadata":{}},{"cell_type":"code","source":"effnetb2 = tf.keras.applications.EfficientNetB2(\n    include_top=False,\n    #weights=\"imagenet\",\n    input_shape=(48,48,3),\n    classes=7\n)\n\nfor layer in effnetb2.layers:\n    layer.trainable=False\nx= Flatten()(effnetb2.output)\n\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel_effnetb2 = Model(inputs=effnetb2.input,outputs=prediction)\nmodel_effnetb2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_effnetb2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size=(48,48),\n                                                 batch_size=128,\n                                                 #shuffle=True,\n                                                 #color_mode=\"grayscale\",\n                                                 class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size=(48,48),\n                                            batch_size=128,\n                                            #shuffle=False,\n                                            #color_mode=\"grayscale\",\n                                            class_mode='categorical')\ntraining_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chk_path = 'effnetb2.h5'\nlog_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             mode='min',\n                             moniter='val_loss')\n\nearlystop = EarlyStopping(monitor='val_loss', \n                          min_delta=0, \n                          patience=3, \n                          restore_best_weights=True)\n                        \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=6, \n                              min_delta=0.0001)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsteps_per_epoch = len(training_set) // 128\nvalidation_steps = len(test_set) // 128\n\nhist_effnetb2 = model_effnetb2.fit_generator(training_set,\n                 validation_data=test_set,\n                 verbose=1,\n                 epochs=60,\n                 callbacks=callbacks,\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### REDO","metadata":{}},{"cell_type":"markdown","source":"## EfficientNetB2","metadata":{}},{"cell_type":"code","source":"pre_model = tf.keras.applications.EfficientNetB2(\n    include_top=False,\n    #weights=\"imagenet\",\n    input_shape=(48,48,3),\n    classes=7\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in pre_model.layers:\n    layer.trainable=False\nlast_output= (pre_model.output)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel = Model(inputs=pre_model.input,outputs=prediction)\n#model.summary()\n\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#steps_per_epoch = len(train_generator) // 256\n#validation_steps = len(test_set) // 256\n\nhistory = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1,\n                   #steps_per_epoch=steps_per_epoch,\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_effnet = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1_effnet.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on EfficientNetB2\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"effnetb2.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DenseNet-121","metadata":{}},{"cell_type":"code","source":"pre_model = tf.keras.applications.DenseNet121(\n    include_top=False,\n    #weights=\"imagenet\",\n    input_shape=(224,224,3),\n    classes=7\n)\nfor layer in pre_model.layers:\n    layer.trainable=False\nlast_output= (pre_model.output)\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel = Model(inputs=pre_model.input,outputs=prediction)\n#model.summary()\n\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#steps_per_epoch = len(train_generator) // 256\n#validation_steps = len(test_set) // 256\n\nhistory = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1,\n                   #steps_per_epoch=steps_per_epoch,\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_densenet = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1_effnet.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1_effnet.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on DenseNet121\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"densenet121.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## InceptionV3","metadata":{}},{"cell_type":"code","source":"pre_model = tf.keras.applications.InceptionV3(\n    include_top=False,\n    #weights=\"imagenet\",\n    input_shape=(299, 299 ,3),\n    classes=7\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in pre_model.layers:\n    layer.trainable=False\nlast_output= (pre_model.output)\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nprediction = Dense(len(folders),activation='softmax')(x)\n\nmodel = Model(inputs=pre_model.input,outputs=prediction)\n#model.summary()\n\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#steps_per_epoch = len(train_generator) // 256\n#validation_steps = len(test_set) // 256\n\nhistory = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1,\n                   #steps_per_epoch=steps_per_epoch,\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_incv3 = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1_incv3.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1_incv3.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1_incv3.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1_incv3.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Inception-V3\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"inceptionv3.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VGG-16","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\npre_model = VGG16(include_top = False,\n                 weights = 'imagenet',\n                 input_shape = (48,48,3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in pre_model.layers:\n    layer.trainable=False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_output = pre_model.output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.backend import sigmoid\n# def swish(x, beta = 1):\n#     return (x * sigmoid(beta * x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.utils.generic_utils import get_custom_objects\n# from keras.layers import Activation\n# get_custom_objects().update({'swish': Activation(swish)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'swish')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Model(pre_model.input, x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 15\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n                                  \nval_datagen = ImageDataGenerator(rescale=1./255)                         \n                                \n#Batch Size and Image Size\nbatch_sz = 256\nsz = 48\n\ntrain_generator = train_datagen.flow_from_directory('../input/fer2013/train',\n                                                    batch_size = batch_sz,\n                                                    class_mode='categorical',\n                                                    target_size=(sz,sz))\n\nval_generator = val_datagen.flow_from_directory('../input/fer2013/test',\n                                                    batch_size = batch_sz,\n                                                    class_mode='categorical',\n                                                    target_size=(sz,sz))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.75):\n#             if(logs.get('accuracy')>78):\n                print(\"\\nReached 74% val_accuracy, so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#opt = tf.keras.optimizers.Adam(learning_rate = lr1)\nopt = tf.keras.optimizers.Adam(learning_rate=lr1)\n\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#steps_per_epoch = len(train_generator) // 256\n#validation_steps = len(test_set) // 256\n\nhistory = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1,\n                   #steps_per_epoch=steps_per_epoch,\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e2=30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1 = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = model.evaluate_generator(val_generator, 256)\n\n\n\nprint(\"[INFO] accuracy: {:.2f}%\".format(test_score[1] * 100)) \n\nprint(\"[INFO] Loss: \",test_score[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap='inferno'):\n\n    \"\"\"\n\n    This function prints and plots the confusion matrix.\n\n    Normalization can be applied by setting `normalize=True`.\n\n    \"\"\"\n\n    plt.figure(figsize=(10,10))\n\n\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n\n    plt.title(title)\n\n    plt.colorbar()\n\n\n\n    tick_marks = np.arange(len(classes))\n\n    plt.xticks(tick_marks, classes, rotation=45)\n\n    plt.yticks(tick_marks, classes)\n\n\n\n    if normalize:\n\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n        cm = np.around(cm, decimals=2)\n\n        cm[np.isnan(cm)] = 0.0\n\n        print(\"Normalized confusion matrix\")\n\n    else:\n\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() / 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n\n        plt.text(j, i, cm[i, j],\n\n                 horizontalalignment=\"center\",\n\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n\n    plt.ylabel('True label')\n\n    plt.xlabel('Predicted label')\n\n\n\n##################################################################################################\n\n\n\n\n\n#Print the Target names\n\n\n\ntarget_names = []\n\nfor key in train_generator.class_indices:\n\n    target_names.append(key)\n\n\n\n# print(target_names)\n\n\n\n#Confution Matrix \n\n\n\nY_pred = model.predict_generator(val_generator)\n\ny_pred = np.argmax(Y_pred, axis=1)\n\nprint('Confusion Matrix')\n\ncm = confusion_matrix(val_generator.classes, y_pred)\n\nplot_confusion_matrix(cm, target_names, title='Confusion Matrix')\n\n\n\n#Print Classification Report\n\nprint('Classification Report')\n\nprint(classification_report(val_generator.classes, y_pred, target_names=target_names))\n\n\n\n#Save the model\n\nmodel.save(\"tutorial.hdf5\")\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('vgg_fer.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('vgg_fer_weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n \n# Convert Keras model to ConcreteFunction\nfull_model = tf.function(lambda x: model(x))\nfull_model = full_model.get_concrete_function(\n    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n \n# Get frozen ConcreteFunction\nfrozen_func = convert_variables_to_constants_v2(full_model)\nfrozen_func.graph.as_graph_def()\n \n# Print out model inputs and outputs\nprint(\"Frozen model inputs: \", frozen_func.inputs)\nprint(\"Frozen model outputs: \", frozen_func.outputs)\n \n# Save frozen graph to disk\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n                  logdir=\"./frozen_models\",\n                  name=\"VGG16_1e-5_10_1e-4_17(gap,32,4).pb\",\n                  as_text=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on VGG-16\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"vgg16.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet50, EffB2, NASNetMobile","metadata":{}},{"cell_type":"code","source":"pre_model = ResNet50(include_top = False,weights=\"imagenet\",input_shape = (224,224,3))\nfor layer in pre_model.layers:\n    layer.trainable=False\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_output = pre_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nmodel = tf.keras.Model(pre_model.input, x)\n#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 15\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 30\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.74):\n#             if(logs.get('accuracy')>78):\n                print(\"\\nReached 74% val_accuracy, so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()\n\nopt = tf.keras.optimizers.Adam(learning_rate = lr1)\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_res = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_res = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MobileNet","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\npre_model = tf.keras.applications.MobileNet(include_top = False,\n                 weights = 'imagenet',\n                 input_shape = (48,48,3))\nfor layer in pre_model.layers:\n    layer.trainable=False\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\npre_model = tf.keras.applications.Xception(include_top = False,\n                 weights = 'imagenet',\n                 input_shape = (71,71,3))\nfor layer in pre_model.layers:\n    layer.trainable=False\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_output = pre_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nmodel = tf.keras.Model(pre_model.input, x)\n#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 15\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 30\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.74):\n#             if(logs.get('accuracy')>78):\n                print(\"\\nReached 74% val_accuracy, so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()\n\nopt = tf.keras.optimizers.Adam(learning_rate = lr1)\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_xception = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_xception = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1_xception.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1_xception.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1_xception.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1_xception.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Xception\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"xception.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1_res.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1_res.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1_res.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1_res.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on ResNet50\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"res50.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=e2\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history1.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history1.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history1.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history1.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on VGG-16\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"vgg16.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history1_xception.epoch, history1_xception.history['accuracy'], label='train')\nsns.lineplot(history1_xception.epoch, history1_xception.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history1_xception.epoch, history1_xception.history['loss'], label='train')\nsns.lineplot(history1_xception.epoch, history1_xception.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.savefig('epoch_history_resnet50.png')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as pyplot\ndf_accu = pd.DataFrame({'train': history1_xception.history['accuracy'], 'valid': history1_xception.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history1_xception.history['loss'], 'valid': history1_xception.history['val_loss']})\n\nfig = pyplot.figure(0, (14, 4))\nax = pyplot.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('performance_vgg16.png')\npyplot.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Redoing VGG","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\npre_model = tf.keras.applications.VGG16(include_top = False,\n                 weights = 'imagenet',\n                 input_shape = (48,48,3))\nfor layer in pre_model.layers:\n    layer.trainable=False\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_output = pre_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nmodel = tf.keras.Model(pre_model.input, x)\n#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 15\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 30\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.74):\n#             if(logs.get('accuracy')>78):\n                print(\"\\nReached 74% val_accuracy, so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()\n\nopt = tf.keras.optimizers.Adam(learning_rate = lr1)\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory_vgg16 = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_vgg16 = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history1_vgg16.epoch, history1_vgg16.history['accuracy'], label='train')\nsns.lineplot(history1_vgg16.epoch, history1_vgg16.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history1_vgg16.epoch, history1_vgg16.history['loss'], label='train')\nsns.lineplot(history1_vgg16.epoch, history1_vgg16.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.savefig('epoch_history_resnet50.png')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as pyplot\ndf_accu = pd.DataFrame({'train': history1_vgg16.history['accuracy'], 'valid': history1_vgg16.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history1_vgg16.history['loss'], 'valid': history1_vgg16.history['val_loss']})\n\nfig = pyplot.figure(0, (14, 4))\nax = pyplot.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('performance_vgg16.png')\npyplot.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_output = pre_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(last_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)\nmodel = tf.keras.Model(pre_model.input, x)\n#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 15\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 30\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.74):\n#             if(logs.get('accuracy')>78):\n                print(\"\\nReached 74% val_accuracy, so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()\n\nopt = tf.keras.optimizers.Adam(learning_rate = lr1)\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_mobile = model.fit(train_generator,\n                   validation_data = val_generator,\n                   validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor layer in model.layers:\n    layer.trainable = True\n#print(model.summary())\nopt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory1_mobile = model.fit(train_generator,\n                    validation_data = val_generator,\n                    validation_steps = val_generator.samples // val_generator.batch_size,\n                    epochs = e2,\n                    callbacks = [callbacks])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history1_mobile.epoch, history1_mobile.history['accuracy'], label='train')\nsns.lineplot(history1_mobile.epoch, history1_mobile.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history1_mobile.epoch, history1_mobile.history['loss'], label='train')\nsns.lineplot(history1_mobile.epoch, history1_mobile.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.savefig('epoch_history_resnet50.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as pyplot\ndf_accu = pd.DataFrame({'train': history1_mobile.history['accuracy'], 'valid': history1_mobile.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history1_mobile.history['loss'], 'valid': history1_mobile.history['val_loss']})\n\nfig = pyplot.figure(0, (14, 4))\nax = pyplot.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('performance_vgg16.png')\npyplot.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapper = {\n    0: \"surprise\",\n    1: \"fear\",\n    2: \"angry\",\n    3: \"neutral\",\n    4: \"sad\",\n    5: \"disgust\",\n    6: \"happy\"\n}\n\nnp.random.seed(2)\nrandom_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\nrandom_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n        ax = pyplot.subplot(2, 9, i+1)\n        sample_img = X_valid[sadidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"true:sad, pred:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        ax = pyplot.subplot(2, 9, i+10)\n        sample_img = X_valid[neuidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:neut, p:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        pyplot.tight_layout()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Xception","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1.0/255.,\n                                   rotation_range=30,\n                                   horizontal_flip=True,\n                                   width_shift_range=0.15,\n                                   height_shift_range=0.15,\n                                   #shear_range=0.15,\n                                   zoom_range=0.15,\n                                   zca_whitening=False,\n#                                    shear_range =20 ,\n                                   brightness_range = [0.8,1.2]\n                                )\n                                  \nval_datagen = ImageDataGenerator(rescale=1./255,\n#                                 shear_range=20\n                                )\n#Batch Size and Image Size\nbatch_sz = 256\nsz = 48\n\ntrain_generator = train_datagen.flow_from_directory('../input/fer2013/train',\n                                                    batch_size = batch_sz,\n                                                    class_mode='categorical',\n                                                    target_size=(sz,sz))\n\nval_generator = val_datagen.flow_from_directory('../input/fer2013/test',\n                                                    batch_size = batch_sz,\n                                                    class_mode='categorical',\n                                                    target_size=(sz,sz))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 7\n\n\ndef xception_model():\n    \n    model=  tf.keras.Sequential()\n    \n    model.add(tf.keras.applications.Xception(input_shape=(71, 71,3),\n                                             include_top = False,\n                                            weights=\"imagenet\", classes=  NUM_CLASSES))\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dense(7, activation='softmax'))\n    opt = tf.keras.optimizers.Adam(lr=0.0001 , decay=1e-6)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xception = xception_model()\nxception.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(xception, to_file='model_1.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chk_path = 'model_1.h5'\nlog_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             #verbose=1,\n                             mode='min',\n                             moniter='val_loss')\n\nearlystop = EarlyStopping(monitor='val_loss', \n                          min_delta=0, \n                          patience=3, \n                          #verbose=1, \n                          restore_best_weights=True)\n                        \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=6, \n                              verbose=1, \n                              min_delta=0.0001)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size =256\nepochs=35\nhistory = xception.fit_generator(\n    train_generator,\n    validation_data = val_generator,\n    validation_steps = val_generator.samples // val_generator.batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history1.epoch, history1.history['accuracy'], label='train')\nsns.lineplot(history1.epoch, history1.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history1.epoch, history1.history['loss'], label='train')\nsns.lineplot(history1.epoch, history1.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.savefig('epoch_history_xception.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom","metadata":{}},{"cell_type":"code","source":"\n\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.models import Model\nimport numpy as np\n\n\nchk_path = 'custom.h5'\nlog_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             mode='min',\n                             moniter='val_loss')\n\nearlystop = EarlyStopping(monitor='val_loss', \n                          min_delta=0, \n                          patience=3, \n                          restore_best_weights=True)\n                        \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=6, \n                              min_delta=0.0001)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]\n\n\nbase_model = VGG16(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(48,48,3),\n    classes=7\n)\n\n#model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n\n#img_path = 'elephant.jpg'\n#img = image.load_img(img_path, target_size=(224, 224))\n#x = image.img_to_array(img)\n#x = np.expand_dims(x, axis=0)\n#x = preprocess_input(x)\n\n#block4_pool_features = model.predict(x)\n\n\nfor layer in base_model.layers:\n  layer.trainable=False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x= Flatten()(xcep.output)\nbase_output = base_model.output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_output.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.GlobalAveragePooling2D()(base_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.Dense(64, kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\nx = tf.keras.layers.LeakyReLU()(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.Dropout(0.25)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx = tf.keras.layers.Dense(32,kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\nx = tf.keras.layers.LeakyReLU()(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx = tf.keras.layers.Dropout(0.25)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npred = tf.keras.layers.Dense(len(folders),activation='softmax',name = 'Output')(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=base_model.input,outputs=pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size=(48,48),\n                                                 batch_size=128,\n                                                 #shuffle=True,\n                                                 #color_mode=\"grayscale\",\n                                                 class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size=(48,48),\n                                            batch_size=128,\n                                            #shuffle=False,\n                                            #color_mode=\"grayscale\",\n                                            class_mode='categorical')\ntraining_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.layers.GlobalAveragePooling2D()(base_output)\nx = tf.keras.layers.Dense(32,activation= 'relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n# x = tf.keras.layers.Dense(16,activation= 'relu')(x)\n# x = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(7,activation='softmax',name = 'Output')(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Model(base_model.input, x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning Rate and Epochs for Model Warm-up\nlr1 = 1e-5\ne1 = 10\n\n#Learning Rate and Epochs for Model Training\nlr2 = 1e-4\ne2 = 17\n\nopt = tf.keras.optimizers.Adam(learning_rate = lr1)\nmodel.compile(optimizer = opt,loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsteps_per_epoch = len(training_set) // 128\nvalidation_steps = len(test_set) // 128\n\nhist1 = model.fit_generator(training_set,\n                 validation_data=test_set,\n                 verbose=1,\n                 epochs=60,\n                 callbacks=callbacks,\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model.layers:\n    layer.trainable = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt1 = tf.keras.optimizers.Adam(learning_rate = lr2)\nmodel.compile(optimizer = opt1, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history1 = model.fit(training_set,\n                    validation_data=test_set,                     \n                    validation_steps =validation_steps,\n                    epochs = e2,\n                    steps_per_epoch=steps_per_epoch,\n                    callbacks = callbacks)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='selu')(x)\n#x = tf.keras.layers.Dropout(0.3)(x)\n#pred = tf.keras.layers.Dense(len(folders),activation='softmax',name = 'Output')(x)\n#kernel_initializer=tf.keras.initializers.GlorotNormal(),\n#model = Model(inputs=base_output,outputs=pred)\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_xcep.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=[0.8,1.2],\n                                   horizontal_flip=True,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2\n                                  )\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size=(48,48),\n                                                 batch_size=128,\n                                                 #shuffle=True,\n                                                 #color_mode=\"grayscale\",\n                                                 class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size=(48,48),\n                                            batch_size=128,\n                                            #shuffle=False,\n                                            #color_mode=\"grayscale\",\n                                            class_mode='categorical')\ntraining_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.applications.xception.preprocess_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsteps_per_epoch = len(training_set) // 128\nvalidation_steps = len(test_set) // 128\n\nhist_xcep = model_xcep.fit_generator(training_set,\n                 validation_data=test_set,\n                 epochs=60,\n                 callbacks=callbacks,\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss and Accuracy plot","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nplt.subplot(1,2,2)\nplt.plot(history1.history['accuracy'])\n#plt.plot(hist_r50.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,1)\nplt.plot(history1.history['loss'])\n#plt.plot(hist_r50.history['val_loss'])\nplt.title('model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"high accracy is achieved on training set but accuracy on validation set is stuck at 66% also no overfitting can se seen in the dataset hence is can be concluded that the inefficiency may be due to the unbalanced dataset","metadata":{}},{"cell_type":"markdown","source":"### Model evaluation","metadata":{}},{"cell_type":"code","source":"train_loss, train_accu = fernet.evaluate(training_set)\ntest_loss, test_accu = fernet.evaluate(test_set)\nprint(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_accu*100, test_accu*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fernet.save_weights('fernet_bestweight.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix and Classification on training set","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(training_set)\ny_pred = np.argmax(y_pred, axis=1)\nclass_labels = test_set.class_indices\nclass_labels = {v:k for k,v in class_labels.items()}\n\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm_train = confusion_matrix(training_set.classes, y_pred)\nprint('Confusion Matrix')\nprint(cm_train)\nprint('Classification Report')\ntarget_names = list(class_labels.values())\nprint(classification_report(training_set.classes, y_pred, target_names=target_names))\n\nplt.figure(figsize=(8,8))\nplt.imshow(cm_train, interpolation='nearest')\nplt.colorbar()\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names, rotation=90)\n_ = plt.yticks(tick_mark, target_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix and Classification on test set","metadata":{}},{"cell_type":"code","source":"y_pred = fernet.predict(test_set)\ny_pred = np.argmax(y_pred, axis=1)\nclass_labels = test_set.class_indices\nclass_labels = {v:k for k,v in class_labels.items()}\n\n#from sklearn.metrics import classification_report, confusion_matrix\ncm_test = confusion_matrix(test_set.classes, y_pred)\nprint('Confusion Matrix')\nprint(cm_test)\nprint('Classification Report')\ntarget_names = list(class_labels.values())\nprint(classification_report(test_set.classes, y_pred, target_names=target_names))\n\nplt.figure(figsize=(8,8))\nplt.imshow(cm_test, interpolation='nearest')\nplt.colorbar()\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names, rotation=90)\n_ = plt.yticks(tick_mark, target_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}